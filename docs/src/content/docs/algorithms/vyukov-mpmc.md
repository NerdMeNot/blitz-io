---
title: Vyukov MPMC Bounded Queue
description: The lock-free multi-producer multi-consumer channel algorithm used in blitz-io, based on Dmitry Vyukov's bounded queue with per-slot sequence numbers.
---

blitz-io's `Channel(T)` implements a bounded MPMC (multi-producer, multi-consumer) queue using Dmitry Vyukov's lock-free ring buffer algorithm. This is the same algorithm used by crossbeam-channel in the Rust ecosystem. The data path (send/receive) is fully lock-free; a separate mutex protects only the waiter lists for async operations.

## The Core Idea

Each slot in the ring buffer has a **sequence number** that encodes the slot's state. Producers and consumers use CAS (compare-and-swap) on shared `head` and `tail` positions to claim slots, then use the sequence number to confirm the slot is in the expected state. No locks are needed on the data path.

```
Ring buffer with capacity 4:

  slot[0]          slot[1]          slot[2]          slot[3]
+----------+     +----------+     +----------+     +----------+
| seq: 0   |     | seq: 1   |     | seq: 2   |     | seq: 3   |
| value: - |     | value: - |     | value: - |     | value: - |
+----------+     +----------+     +----------+     +----------+
  ^                                                  ^
  head=0                                             tail=0
```

Initially, each slot's sequence equals its index. This means all slots are "writable" from the producer's perspective.

## The Algorithm

### Send (Producer)

To send a value at position `tail`:

1. Load the current `tail` position.
2. Compute `idx = tail % capacity`.
3. Load `slot[idx].sequence`.
4. Compare `sequence` with `tail`:
   - If `sequence == tail`: The slot is writable. CAS `tail` to `tail + 1` to claim it.
   - If `sequence < tail` (as signed difference): The slot still has unconsumed data. Queue is **full**.
   - If `sequence > tail`: Another producer claimed this slot. Reload `tail` and retry.
5. After claiming: write the value, then store `sequence = tail + 1` (marking "has data").

```zig
pub fn trySend(self: *Self, value: T) SendResult {
    if (self.closed_flag.load(.acquire)) return .closed;

    var tail = self.tail.load(.monotonic);
    while (true) {
        const idx: usize = @intCast(tail % self.buf_cap);
        const slot = &self.slots[idx];
        const seq = slot.sequence.load(.acquire);

        const diff: i64 = @bitCast(seq -% tail);

        if (diff == 0) {
            // Slot writable -- claim by advancing tail
            if (self.tail.cmpxchgWeak(tail, tail +% 1, .acq_rel, .monotonic)) |new_tail| {
                tail = new_tail;
                continue;
            }
            // Claimed! Write value and publish
            self.buffer[idx] = value;
            slot.sequence.store(tail +% 1, .release);

            if (self.has_recv_waiters.load(.acquire)) {
                self.wakeOneRecvWaiter();
            }
            return .ok;
        } else if (diff < 0) {
            return .full;
        } else {
            tail = self.tail.load(.monotonic);
        }
    }
}
```

### Receive (Consumer)

To receive a value at position `head`:

1. Load the current `head` position.
2. Compute `idx = head % capacity`.
3. Load `slot[idx].sequence`.
4. Compare `sequence` with `head + 1`:
   - If `sequence == head + 1`: The slot has data. CAS `head` to `head + 1` to claim it.
   - If `sequence < head + 1` (as signed difference): The slot is empty. Queue is **empty**.
   - If `sequence > head + 1`: Another consumer claimed this slot. Reload `head` and retry.
5. After claiming: read the value, then store `sequence = head + capacity` (marking "writable again").

```zig
pub fn tryRecv(self: *Self) RecvResult {
    var head = self.head.load(.monotonic);
    while (true) {
        const idx: usize = @intCast(head % self.buf_cap);
        const slot = &self.slots[idx];
        const seq = slot.sequence.load(.acquire);

        const diff: i64 = @bitCast(seq -% (head +% 1));

        if (diff == 0) {
            // Slot has data -- claim by advancing head
            if (self.head.cmpxchgWeak(head, head +% 1, .acq_rel, .monotonic)) |new_head| {
                head = new_head;
                continue;
            }
            // Claimed! Read value and release slot
            const value = self.buffer[idx];
            slot.sequence.store(head +% self.buf_cap, .release);

            if (self.has_send_waiters.load(.acquire)) {
                self.wakeOneSendWaiter();
            }
            return .{ .value = value };
        } else if (diff < 0) {
            if (self.closed_flag.load(.acquire)) return .closed;
            return .empty;
        } else {
            head = self.head.load(.monotonic);
        }
    }
}
```

## Sequence Number State Machine

The sequence number cycles through states as the slot is used. For a slot at index `i` with channel capacity `C`:

```
                            Producer claims
Writable: seq = i    --->   Has data: seq = tail + 1
    ^                            |
    |                            | Consumer claims
    |                            v
    +--- seq = head + C ---  Read complete
```

Each full cycle advances the sequence by `C`. After `N` complete send/receive cycles, the sequence for slot `i` is `i + N * C`. The wrapping arithmetic ensures this works correctly even when positions overflow u64.

### Visual Walkthrough

Starting state (capacity 4, all slots writable):

```
slot[0].seq=0  slot[1].seq=1  slot[2].seq=2  slot[3].seq=3
head=0, tail=0
```

After `send(A)` at tail=0:

```
slot[0].seq=1  slot[1].seq=1  slot[2].seq=2  slot[3].seq=3
         ^--- has data (seq == old_tail + 1)
head=0, tail=1
```

After `send(B)` at tail=1:

```
slot[0].seq=1  slot[1].seq=2  slot[2].seq=2  slot[3].seq=3
head=0, tail=2
```

After `recv()` at head=0, returns A:

```
slot[0].seq=4  slot[1].seq=2  slot[2].seq=2  slot[3].seq=3
         ^--- writable again (seq = head + capacity = 0 + 4)
head=1, tail=2
```

## ABA Prevention Through Sequence Numbers

The classic ABA problem in lock-free data structures occurs when a CAS succeeds even though the value was changed and then changed back. The Vyukov queue is immune to ABA because sequence numbers monotonically increase.

Consider the scenario:
1. Thread A reads `head=5`, `slot[5 % C].seq = 6` (data ready, `6 == 5+1`).
2. Thread A is preempted.
3. Thread B receives from head=5, advances head to 6. Slot 5's seq becomes `5 + C`.
4. Thread B sends again, slot wraps, seq becomes `5 + C + 1`.
5. Thread A resumes, tries CAS on head from 5 to 6.

At step 5, `head` is now 6, not 5. Thread A's CAS fails. Even if by some contortion `head` wrapped back to 5, the sequence number would be `5 + C` (writable), not `6` (has data), so Thread A would see `diff != 0` and retry. The sequence counter makes each slot visit unique.

## Cache Line Padding

The `head` and `tail` fields are on separate cache lines to prevent false sharing between producers and consumers:

```zig
head: std.atomic.Value(u64) align(CACHE_LINE_SIZE),
tail: std.atomic.Value(u64) align(CACHE_LINE_SIZE),
```

Without this padding, every CAS on `head` would invalidate the cache line containing `tail` on other cores, and vice versa. Since producers write `tail` and consumers write `head`, false sharing would cause severe performance degradation under contention.

The slot metadata (sequence counters) and value storage are separate arrays:

```zig
slots: []Slot,      // Sequence counters only (small, cache-friendly)
buffer: []T,        // Values (potentially large)
```

This separation improves cache utilization because the hot path (checking sequence numbers) only touches the compact `slots` array.

## Minimum Buffer Size

The internal buffer size is always at least 2, even if the user requests capacity 1:

```zig
const buf_cap: u64 = @max(cap, 2);
```

This is required for correctness. With a single slot (buf_cap=1), the sequence number after a send would be `tail + 1 = 1`, and after a receive it would be `head + capacity = 0 + 1 = 1`. The "has data" state and "writable" state would have the same sequence number, making them indistinguishable.

For capacity-1 channels with buf_cap=2, an explicit capacity check ensures the channel does not exceed the user-requested capacity:

```zig
if (self.buf_cap != self.capacity) {
    const head = self.head.load(.acquire);
    if (tail -% head >= self.capacity) return .full;
}
```

## Waiter Integration for Async Operations

The lock-free ring buffer handles the fast path. When the channel is full (for senders) or empty (for receivers), tasks must wait. This is handled by a separate waiter system that does not touch the lock-free data path:

```
Fast path (lock-free):          Slow path (mutex-protected):
  trySend() / tryRecv()           waiter_mutex
  CAS on head/tail                send_waiters: intrusive list
  sequence number check           recv_waiters: intrusive list
```

The waiter system uses a "flag-before-check" protocol to prevent lost wakeups:

1. Lock `waiter_mutex`.
2. Set the `has_recv_waiters` (or `has_send_waiters`) flag.
3. Re-check the ring buffer under lock (`trySend`/`tryRecv`).
4. If the re-check succeeds, clear the flag and return immediately.
5. If still blocked, add the waiter to the list and release the lock.

After a successful `trySend`, the sender checks `has_recv_waiters` and wakes one receiver if set. After a successful `tryRecv`, the receiver checks `has_send_waiters` and wakes one sender. This ensures no waiter is stranded.

The wakeup avoids use-after-free by copying the waker function pointer and context **before** setting the `complete` flag:

```zig
fn wakeOneRecvWaiter(self: *Self) void {
    self.waiter_mutex.lock();
    const waiter = self.recv_waiters.popFront();
    // ...
    self.waiter_mutex.unlock();

    if (waiter) |w| {
        // Copy waker info BEFORE setting complete (avoids use-after-free)
        const waker_fn = w.waker;
        const waker_ctx = w.waker_ctx;
        w.complete.store(true, .release);  // Owner may destroy waiter after this
        if (waker_fn) |wf| {
            if (waker_ctx) |ctx| {
                wf(ctx);
            }
        }
    }
}
```

## Comparison with Other Approaches

| Approach | Send | Recv | Bounded | Lock-free |
|----------|------|------|---------|-----------|
| Vyukov bounded (blitz-io) | O(1) CAS | O(1) CAS | Yes | Yes (data path) |
| Mutex + VecDeque | O(1) + lock | O(1) + lock | Optional | No |
| Michael-Scott queue | O(1) CAS + alloc | O(1) CAS + free | No | Yes |
| LCRQ (linked CRQs) | O(1) CAS | O(1) CAS | No | Yes |

The Vyukov bounded queue is ideal for blitz-io's use case:
- **Bounded**: provides backpressure (senders wait when full), preventing unbounded memory growth.
- **Lock-free data path**: no mutex on send/receive, only on waiter management.
- **No allocation per operation**: the ring buffer is pre-allocated.
- **Efficient under both low and high contention**: low contention means CAS succeeds on first try; high contention means retry loops converge quickly because successful CAS operations advance the position.

### Source Files

- Channel implementation: `/Users/srini/Code/blitz-io/src/channel/Channel.zig`
